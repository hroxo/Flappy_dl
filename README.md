# Flappy Bird RL â€“ Jogo + Deep Learning em Python

Projeto em Python que recria o jogo **Flappy Bird** com duas modalidades:
- ğŸ•¹ï¸ **Modo Jogador Humano** â€“ jogas com o teclado.
- ğŸ¤– **Modo IA (Deep RL)** â€“ um agente com rede neural aprende a jogar sozinho.
- ğŸ‘€ **Treino em tempo real** â€“ opÃ§Ã£o para veres o agente a treinar â€œao vivoâ€.

O objetivo deste projeto Ã© ser **uma peÃ§a forte de portefÃ³lio**, combinando:
- ProgramaÃ§Ã£o de jogos com `pygame`
- Conceitos de **Reinforcement Learning** (DQN)
- EstruturaÃ§Ã£o de cÃ³digo limpa e modular
- VisualizaÃ§Ã£o clara da evoluÃ§Ã£o do agente

---

## ğŸ”§ Tecnologias usadas

- **Python 3.10+** (recomendado)
- [Pygame](https://www.pygame.org/)
- [PyTorch](https://pytorch.org/)
- `numpy`

Mais tarde, poderÃ¡s acrescentar:
- `matplotlib` (para grÃ¡ficos de evoluÃ§Ã£o de score)
- `tqdm` (barra de progresso no treino)

---

## ğŸ“ Estrutura do projeto

Estrutura sugerida:

```text
flappy-bird-rl/
â”‚
â”œâ”€ game/
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ bird.py           # classe Bird (fÃ­sica e desenho)
â”‚   â”œâ”€ pipes.py          # classe Pipe(s) e gestÃ£o dos obstÃ¡culos
â”‚   â””â”€ game_core.py      # lÃ³gica do jogo + ambiente RL (FlappyEnv)
â”‚
â”œâ”€ rl/
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ dqn_agent.py      # definiÃ§Ã£o da rede neural + agente DQN
â”‚   â””â”€ replay_buffer.py  # memÃ³ria de experiÃªncias (replay buffer)
â”‚
â”œâ”€ assets/               # imagens, fontes, etc. (opcional)
â”‚
â”œâ”€ main_human.py         # jogar Flappy Bird como humano
â”œâ”€ main_train.py         # treinar a IA (DQN) no ambiente
â”œâ”€ main_watch.py         # ver o agente treinado a jogar
â”‚
â”œâ”€ requirements.txt
â””â”€ README.md
````

---

## ğŸš€ InstalaÃ§Ã£o

1. **Clonar o repositÃ³rio**

```bash
git clone https://github.com/<o-teu-username>/flappy-bird-rl.git
cd flappy-bird-rl
```

2. **Criar e ativar ambiente virtual (opcional, mas recomendado)**

```bash
python -m venv venv

# Linux/macOS
source venv/bin/activate

# Windows
venv\Scripts\activate
```

3. **Instalar dependÃªncias**

```bash
pip install -r requirements.txt
```

Exemplo de `requirements.txt` (podes ir ajustando):

```text
pygame
torch
numpy
matplotlib
tqdm
```

---

## âœ… Passos para desenvolver o projeto

### 1. Implementar o jogo base (modo humano)

**Objetivo:** ter um Flappy Bird jogÃ¡vel por um humano, sem IA.

Tarefas:

* Criar a janela com `pygame` (por ex. 400x600).
* Implementar a classe `Bird` (`game/bird.py`):

  * posiÃ§Ã£o (x, y)
  * velocidade vertical
  * gravidade
  * mÃ©todo `flap()` que aplica impulso para cima
  * mÃ©todos `update()` e `draw(screen)`
* Implementar os `Pipes` (`game/pipes.py`):

  * geraÃ§Ã£o de pares de canos (topo/fundo) com um gap
  * movimento para a esquerda
  * reciclagem quando saem do ecrÃ£
* Implementar lÃ³gica de jogo em `game_core.py`:

  * detetar colisÃµes (bird vs pipes, bird vs chÃ£o/teto)
  * gerir o score (incrementa quando passas um par de canos)

Criar `main_human.py` para:

* Ler input do teclado (ex.: tecla espaÃ§o â†’ `bird.flap()`).
* Atualizar bird e pipes a cada frame.
* Desenhar tudo no ecrÃ£.
* Mostrar score atual.

Quando esta fase estiver feita, tens um jogo Flappy Bird bÃ¡sico.

---

### 2. Transformar o jogo num â€œambienteâ€ RL

**Objetivo:** criar uma interface tipo OpenAI Gym para a IA poder interagir.

No ficheiro `game/game_core.py`, criar uma classe:

```python
class FlappyEnv:
    def __init__(self, render: bool = False):
        ...

    def reset(self):
        """
        Reinicia o jogo e devolve o estado inicial.
        """
        return state

    def step(self, action: int):
        """
        Executa uma aÃ§Ã£o:
          action = 0 -> nÃ£o faz nada
          action = 1 -> flap
        AvanÃ§a o jogo um passo (frame) e devolve:
          next_state, reward, done, info
        """
        return next_state, reward, done, info

    def render(self):
        """
        Desenha o estado atual no ecrÃ£ (se render estiver ativo).
        """
        ...
```

#### Definir o estado (state)

SugestÃ£o de features:

* `bird_y` â€“ posiÃ§Ã£o vertical do pÃ¡ssaro
* `bird_velocity` â€“ velocidade vertical
* `dist_x` â€“ distÃ¢ncia horizontal atÃ© ao prÃ³ximo cano
* `gap_y` â€“ posiÃ§Ã£o vertical do gap do prÃ³ximo cano
* opcional: `bird_y - gap_y`

O `state` pode ser um `numpy.array` com estes valores (normalizados se necessÃ¡rio).

#### Definir as aÃ§Ãµes

* `0` = nÃ£o fazer nada (deixar o pÃ¡ssaro cair por gravidade).
* `1` = flap (aplicar impulso para cima).

#### Recompensa (reward)

Exemplo simples:

* `+0.1` por cada frame vivo.
* `+1` (ou +5) por cada cano passado.
* `-1` no momento em que o pÃ¡ssaro morre (colisÃ£o).

---

### 3. Implementar o agente DQN

Na pasta `rl/`:

#### 3.1. Rede neural (PyTorch) â€“ `dqn_agent.py`

* Input: tamanho igual ao nÃºmero de features do estado.
* 2â€“3 camadas fully-connected com ReLU.
* Output: 2 valores â†’ Q(s, aÃ§Ã£o=0) e Q(s, aÃ§Ã£o=1).

Criar uma classe `DQNAgent` com:

* `select_action(state, epsilon)` â€“ polÃ­tica Îµ-greedy.
* `optimize_model()` â€“ um passo de treino a partir do replay buffer.
* gestÃ£o de duas redes (online e target), se quiseres fazer DQN â€œclÃ¡ssicoâ€.

#### 3.2. Replay Buffer â€“ `replay_buffer.py`

Classe para guardar experiÃªncias:

```python
(state, action, reward, next_state, done)
```

Com mÃ©todos:

* `push(...)` â€“ adicionar transiÃ§Ã£o
* `sample(batch_size)` â€“ amostrar batch aleatÃ³rio

---

### 4. Treinar a IA (`main_train.py`)

**Objetivo:** loop de treino de RL com DQN.

Pseudo-cÃ³digo:

```python
env = FlappyEnv(render=VER_TREINO_AO_VIVO_OU_NAO)
agent = DQNAgent(...)
buffer = ReplayBuffer(...)

for episode in range(num_episodes):
    state = env.reset()
    done = False
    total_reward = 0

    while not done:
        # 1. Escolher aÃ§Ã£o (Îµ-greedy)
        action = agent.select_action(state, epsilon)

        # 2. Executar aÃ§Ã£o no ambiente
        next_state, reward, done, info = env.step(action)

        # 3. Guardar transiÃ§Ã£o no replay buffer
        buffer.push(state, action, reward, next_state, done)

        # 4. Atualizar rede
        agent.optimize_model(buffer)

        state = next_state
        total_reward += reward

        # 5. Render opcional (para ver o treino ao vivo)
        if RENDER_DURING_TRAIN:
            env.render()

    # Atualizar epsilon ao longo dos episÃ³dios
    # Guardar melhor modelo com base no score
```

Durante o treino, podes:

* Imprimir no terminal:

  * episÃ³dio, score, melhor score
* Mostrar na janela (usando Pygame):

  * episÃ³dio atual
  * epsilon
  * score atual
  * best score

---

### 5. Ver o agente treinado (`main_watch.py`)

Neste script:

* Carregar o modelo treinado (ficheiro `.pt` ou `.pth`).
* Criar `FlappyEnv(render=True)`.
* Correr um loop onde a aÃ§Ã£o Ã© sempre:

  * `argmax(Q(state))` (sem exploraÃ§Ã£o random).
* NÃ£o Ã© preciso replay buffer nem treino, apenas **inferÃªncia**.

Isto Ã© o â€œmodo espetÃ¡culoâ€ para mostrar no portefÃ³lio.

---

## ğŸ•¹ï¸ Como correr

### Jogar como humano

```bash
python main_human.py
```

### Treinar a IA

```bash
python main_train.py
```

### Ver a IA a jogar (modelo jÃ¡ treinado)

```bash
python main_watch.py
```

---

## ğŸ“Š Melhorias futuras / Roadmap

* [ ] Guardar e plotar grÃ¡ficos de:

  * score por episÃ³dio
  * mÃ©dia mÃ³vel dos Ãºltimos N episÃ³dios
* [ ] Suporte a diferentes esquemas de recompensa
* [ ] Ajustar hiperparÃ¢metros (learning rate, Î³, tamanho do replay, etc.)
* [ ] Adicionar menu inicial para escolher:

  * jogar humano
  * treinar IA
  * ver IA treinada
* [ ] OtimizaÃ§Ãµes de performance quando o render estÃ¡ desligado (treino mais rÃ¡pido)

---

## ğŸ“ Conceitos abordados (para CV/portefÃ³lio)

* ProgramaÃ§Ã£o de jogos com **Pygame**
* Conceitos de fÃ­sica simples (gravidade, velocidade)
* Design de **ambientes RL** (reset/step/state/reward)
* ImplementaÃ§Ã£o de **DQN** com PyTorch
* Uso de **Replay Buffer** e polÃ­tica Îµ-greedy
* OrganizaÃ§Ã£o modular de projeto em Python
